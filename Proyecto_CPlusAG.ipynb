{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V48Fwk9Ix9CQ"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "“Nonlinear Conjugate Gradient for Smooth Convex Functions” de Sahar Karimi y Stephen Vavasis propone un método llamado C+AG (conjugate plus accelerated gradient) que intenta cerrar la brecha entre el método de gradiente conjugado no lineal (NCG), óptimo para funciones cuadráticas, y el método de gradiente acelerado (AG), óptimo para funciones convexas suaves.\n",
        "\n",
        "El método de Gradiente Conjugados no lineales (NCG) es ampliamente utilizado para problemas de optimización sin restricciones, sin embargo, en los casos en que es aplicado a funciones convexas suaves sólo satisface límites de complejidad débiles.\n",
        "\n",
        "Por otro lado, el método del Gradiente Acelerado de Nesterov (AG) es un método óptimo para esta clase de funciones (convexas suaves), pero no es tan eficiente como NCG para funciones cuadráticas:\n",
        "\n",
        "* para el caso de las funciones cuadráticas, la decisión más apropiada sería utilizar un método de Gradiente Conjugado, por tanto es claro que existe una brecha en las opciones de los algoritmos de optimización disponibles.\n",
        "\n",
        "* NCG es la solución óptima para funciones cuadráticas y de hecho demuestra un desempeño bastante apropiado para funciones generales, sin embargo tiene límites de complejidad bastante pobres comparados con AG.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIJxyNmsWsNU"
      },
      "source": [
        "## Gradiente Conjugado + Acelerado\n",
        "\n",
        "El método de Gradiente Conjugado + Acelerado (C+AG) nace como una propuesta para cerrar esta brecha en nuestras opciones para la solución de los problemas anteriormente mencionados, busca ser óptimo para funciones cuadráticas además de satisfacer los mejores límites de complejidad para funciones convexas suaves más generales.\n",
        "\n",
        "Idea:\n",
        "* Seguir los pasos del método de Gradiente Conjugado (hasta que el progreso comience a ser insuficiente)\n",
        "* Cambiar los pasos a los del método de Gradiente Acelerado\n",
        "* Al final regresar al Gradiente Conjugado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFZ-LmsWsNV"
      },
      "source": [
        "## Objetivos de la implementación\n",
        "\n",
        "Implementar los métodos C+AG, AG, y NCG, y encontrar ciertos casos específicos en los cuáles el método de C+AG supera a ambos métodos AG y NCG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U0Fom-hyAzQ"
      },
      "source": [
        "# Fundamentos teóricos\n",
        "El problema bajo consideración es minimizar una función L-Suave convexa $f: \\mathcal{R}^{n} → \\mathcal{R}$ sin restricciones.\n",
        "\n",
        "* $\\textbf{Definicion:}$ Una función es L-Convexa $\\forall \\; x,y \\in \\; \\mathcal{R}^{n}$ si:\n",
        "\n",
        "\\begin{equation}\n",
        "  f(y) - f(x) - \\nabla f(x)^{T} (y-x) \\leq L ||x-y||^{2}/2\n",
        "\\end{equation}\n",
        "\n",
        "También se considerarán funciones l-fuertemente convexas.\n",
        "\n",
        "* $\\textbf{Definicion:}$ Una función es l-fuertemente convexa si $\\forall \\; x,y \\in \\; \\mathcal{R}^{n}$ si:\n",
        "\n",
        "\\begin{equation}\n",
        "  f(y) - f(x) - \\nabla f(x)^{T} (y-x) \\geq l ||x-y||^{2}/2\n",
        "\\end{equation}\n",
        "\n",
        "Para **funciones L-convexas suaves** está probado que el método de gradiente acelerado requiere de $O(-\\epsilon^{-1/2})$ iteraciones para alcanzar una precisión deseada de $\\epsilon$.\n",
        "\n",
        "Sin embargo, teóricamente NCG no tiene límites de complejidad óptimos para funciones convexas suaves generales, de modo que podría necesitar un mayor número de iteraciones que el método AG para este tipo de funciones.\n",
        "\n",
        "Para **funciones l-fuertemente convexas** está probado que el método de gradiente acelerado requiere de $O(\\sqrt{L/l} \\: ln(1/\\epsilon))$ iteraciones para alcanzar una precisión deseada de $\\epsilon$.\n",
        "\n",
        "De igual manera, teóricamente NCG no tiene límites de complejidad óptimos para esta clase de funciones, dando como resultado un posible peor desempeño que el método de AG.\n",
        "\n",
        "Sin embargo, el método de C+AG propuesto por los autores cumple los límites de complejidad del método AG para **funciones L-convexas suaves** y para **funciones l-fuertemente** convexas, es decir $O(-\\epsilon^{-1/2})$ y $O(\\sqrt{L/l} \\: ln(1/\\epsilon))$, siendo una solución óptima para este tipo de problemas.\n",
        "\n",
        "Además, mantiene la característica del método NCG de ser una solución óptima para las funciones cuadráticas, a diferencia del método AG. Por tanto vemos que el método propuesto C+AG combina los beneficios de ambos métodos manteniendo límites de complejidad óptimos para funciones generales y siendo también una solución óptima para funciones cuadráticas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUFQthGHWsNX"
      },
      "source": [
        "## ¿Cómo C+AG logra cerrar esta brecha?\n",
        "\n",
        "1. **Transición dinámica entre NCG y AG:** C+AG inicia con los pasos del gradiente conjugado no lineal, los cuáles son óptimos para solucionar funciones cuadráticas, lo que provoca que para estos casos se haga un progreso rápido.\n",
        "En el caso en que no se el progreso inicial no sea suficiente entonces se cambian los pasos a los de gradiente acelerado para asegurarse de mantener los límites de complejidad del mismo y reducir el número de iteraciones necesarias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpKaTGwfWsNX"
      },
      "source": [
        "2. **Medida de progreso basada en Nesterov:** C+AG utiliza una medida de progreso basada en la secuencia de estimación de AG (Nesterov), con esto podemos evaluar si el progreso realizado es suficiente. Esta medida se basa en una secuencia de funciones cuadráticas fuertemente convexas $\\phi_{k}(x)$, llamada secuencia de estimación.\n",
        "\n",
        "  Primero inicializamos la secuencia:\n",
        "\n",
        "  \\begin{equation}\n",
        "    \\phi_{0}(x) = f(x_{0}) + \\frac{L}{2}||x - v_{0}||^{2}\n",
        "  \\end{equation}\n",
        "\n",
        "  donde $v_{0} = x_{0}$ y $L$ es la constante de suavidad de f.\n",
        "\n",
        "  Después, para $k \\geq 1$ la secuencia se actualiza de la siguiente forma:\n",
        "\n",
        "  \\begin{equation}\n",
        "    \\phi_{k+1}(x) = (1 - \\theta_{k})\\phi_{k}(x) + \\theta_{k} \\left[ f(\\bar{x}_{k}) + \\nabla f(\\bar{x}_{k})^{T}(x - \\bar{x}_{k}) + \\frac{l}{2}||x - \\bar{x}_{k}||^{2}\\right]\n",
        "  \\end{equation}\n",
        "\n",
        "  donde $\\phi_{k} \\in (0,1)$ se elige como la raíz positiva de la ecuación cuadrática:\n",
        "\n",
        "  \\begin{equation}\n",
        "    L \\theta_{k}^{2} + (\\gamma_{k} - l)\\theta_{k} - \\gamma_{k} = 0\n",
        "  \\end{equation}\n",
        "\n",
        "  donde $\\gamma_{k}$ es un parámetro auxiliar que se actualiza cada iteración de la forma:\n",
        "\n",
        "  \\begin{equation}\n",
        "    \\gamma_{k+1} = (1- \\theta_{k}) \\gamma_{k} + \\theta_{k} l\n",
        "  \\end{equation}\n",
        "\n",
        "  Y $\\bar{x}_{k}$ es el punto donde se evalúa el gradiente en la iteración k. En la mayoría de los casos $\\bar{x}_{k}$ es simplemente el punto actual $x_{k}$.\n",
        "\n",
        "  Después de calcular la secuencia, realizamos el test de progreso suficiente para determinar si el método debe continuar con gradiente conjugado o si debe cambiar a gradiente acelerado. El test se basa en verficar si el valor de la función objetivo $f(x_{k})$ es menor o igual al valor de la función $\\phi_{k}(x_{k})$, si esta condición se cumple, entonces consideramos que ha hecho suficiente progreso:\n",
        "\n",
        "  \\begin{equation}\n",
        "    f(x_{k}) \\leq \\phi_{k}^{*}\n",
        "  \\end{equation}\n",
        "\n",
        "  Donde $\\phi_{k}^{*}$ es el mínimo de la función $\\phi_{k}(x)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyr3L_krWsNY"
      },
      "source": [
        "3. **Reinicio del gradiente conjugado:** En casos cuando el gradiente conjugado no haga suficiente progreso, el algoritmo utiliza un reinicio con un paso de descenso más pronunciado, con esto es seguro que el algoritmo no se estanque y siga avanzando.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4XjLJMWWsNZ"
      },
      "source": [
        "4. **Evaluación adaptativa del tamaño de paso y los parámetros:** Con el método se incluye una función para estimar el parámetro L de forma adaptativa en caso de que este no se conozca a priori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi5efI-PyB0i"
      },
      "source": [
        "# Implementación de C+AG en python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcd0cIpZx3OJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def ComputeThetaGamma(L, ell, gamma_k):\n",
        "\n",
        "    a = L\n",
        "    b = gamma_k - ell\n",
        "    c = -gamma_k\n",
        "    theta_k = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
        "    gamma_k_plus_1 = (1 - theta_k) * gamma_k + theta_k * ell\n",
        "\n",
        "    return theta_k, gamma_k_plus_1\n",
        "\n",
        "def EstimateL(f, x0, f0, grad, L):\n",
        "\n",
        "    for _ in range(60):\n",
        "        f1 = f(x0 - grad / L)\n",
        "        if f1 >= f0 - np.dot(grad, grad) / (2 * L) and abs(f1 - f0) >= 1e-11 * abs(f0):\n",
        "            L *= np.sqrt(2)\n",
        "        else:\n",
        "            return L\n",
        "\n",
        "    raise ValueError(\"Line search failed to determine L; possible incorrect gradient function or excessive roundoff error\")\n",
        "\n",
        "def EstimateLInitial(f, x0, grad, L_init=1.0):\n",
        "\n",
        "    L = L_init\n",
        "    for _ in range(100):\n",
        "        if f(x0 - grad / L) < f(x0) - np.dot(grad, grad) / (2 * L):\n",
        "            L /= np.sqrt(2)\n",
        "        else:\n",
        "            return EstimateL(f, x0, f(x0), grad, L)\n",
        "\n",
        "    raise ValueError(\"f may be unbounded below\")\n",
        "\n",
        "def ComputeVPhiStar(theta_k, gamma_k, gamma_k_plus_1, ell, v_k, phi_star_k, x_bar_k, f_x_bar_k, grad_x_bar_k):\n",
        "\n",
        "    v_k_plus_1 = (1 / gamma_k_plus_1) * ((1 - theta_k) * gamma_k * v_k + theta_k * ell * x_bar_k - theta_k * grad_x_bar_k)\n",
        "\n",
        "    phi_star_k_plus_1 = (1 - theta_k) * phi_star_k + theta_k * f_x_bar_k - (theta_k**2 / (2 * gamma_k_plus_1)) * np.dot(grad_x_bar_k, grad_x_bar_k) + \\\n",
        "                        (theta_k * (1 - theta_k) * gamma_k / gamma_k_plus_1) * (ell * np.dot(x_bar_k - v_k, x_bar_k - v_k) / 2 + np.dot(grad_x_bar_k, v_k - x_bar_k))\n",
        "\n",
        "    return v_k_plus_1, phi_star_k_plus_1\n",
        "\n",
        "def CPlusAG(f, grad, x0, ell=0, L=np.nan, gtol=1e-8):\n",
        "    # Inicialización\n",
        "\n",
        "    f0 = f(x0)\n",
        "    grad0 = grad(x0)\n",
        "    phi_star_0 = f0\n",
        "    v0 = x0\n",
        "    only_ag = False\n",
        "    icg = 0\n",
        "    iag = 0\n",
        "    p0 = -grad0\n",
        "    LNaNFlag = np.isnan(L)\n",
        "    if LNaNFlag:\n",
        "        L = EstimateLInitial(f, x0, grad0)\n",
        "        ell = 0\n",
        "    gamma0 = L\n",
        "    xk = x0\n",
        "    vk = v0\n",
        "    phi_star_k = phi_star_0\n",
        "    pk = p0\n",
        "\n",
        "    # Ciclo de iteraciones\n",
        "    for k in range(int(1e6)):\n",
        "        theta_k, gamma_k_plus_1 = ComputeThetaGamma(L, ell, gamma0)\n",
        "        for whichsteptype in range(1, 4):\n",
        "            # Pasos de Gradiente Conjugado (GC)\n",
        "            if whichsteptype <= 2:\n",
        "\n",
        "                if only_ag:\n",
        "                    continue\n",
        "\n",
        "                if icg >= 6 * len(x0) + 1 or whichsteptype == 2:\n",
        "                    pk = -grad(xk)\n",
        "                    icg = 0\n",
        "\n",
        "                if icg == 0 and k > 0 and LNaNFlag:\n",
        "                    L = EstimateL(f, xk, f(xk), grad(xk), L)\n",
        "\n",
        "                icg += 1\n",
        "                iag = 0\n",
        "                x_tilde = xk + pk / L\n",
        "                grad_tilde = grad(x_tilde)\n",
        "\n",
        "                if np.linalg.norm(grad_tilde) <= gtol:\n",
        "                    return x_tilde, k, True\n",
        "\n",
        "                Ap = L * (grad_tilde - grad(xk))\n",
        "                pAp = np.dot(pk, Ap)\n",
        "\n",
        "                if np.dot(grad(xk), pk) >= 0 or pAp <= 0:\n",
        "                    continue\n",
        "\n",
        "                alpha_k = -np.dot(grad(xk), pk) / pAp\n",
        "                xk_plus_1 = xk + alpha_k * pk\n",
        "                fk_plus_1 = f(xk_plus_1)\n",
        "                gradk_plus_1 = grad(xk_plus_1)\n",
        "\n",
        "                if np.linalg.norm(gradk_plus_1) <= gtol:\n",
        "                    return xk_plus_1, k, True\n",
        "                vk_plus_1, phi_star_k_plus_1 = ComputeVPhiStar(theta_k, gamma0, gamma_k_plus_1, ell, vk, phi_star_k, xk, f(xk), grad(xk))\n",
        "\n",
        "                if fk_plus_1 <= phi_star_k_plus_1:\n",
        "                    y_hat = gradk_plus_1 - grad(xk)\n",
        "                    beta_1 = np.dot((y_hat - pk * 2 * np.dot(y_hat, y_hat) / np.dot(y_hat, pk)), gradk_plus_1) / np.dot(y_hat, pk)\n",
        "                    beta_2 = -1 / np.linalg.norm(pk) * min(0.01 * np.linalg.norm(grad0), np.linalg.norm(gradk_plus_1))\n",
        "                    beta_k_plus_1 = max(beta_1, beta_2)\n",
        "                    pk_plus_1 = -gradk_plus_1 + beta_k_plus_1 * pk\n",
        "                    pk = pk_plus_1\n",
        "                    xk = xk_plus_1\n",
        "                    vk = vk_plus_1\n",
        "                    phi_star_k = phi_star_k_plus_1\n",
        "                    break\n",
        "            # Pasos de Gradiente Acelerado (AG)\n",
        "            else:\n",
        "                if not only_ag:\n",
        "                    only_ag = True\n",
        "                    iag = 0\n",
        "                    icg = 0\n",
        "\n",
        "                iag += 1\n",
        "                x_bar_k = (theta_k * gamma0 * vk + gamma_k_plus_1 * xk) / (gamma0 + theta_k * ell)\n",
        "                grad_bar_k = grad(x_bar_k)\n",
        "\n",
        "                if np.linalg.norm(grad_bar_k) <= gtol:\n",
        "                    return x_bar_k, k, True\n",
        "\n",
        "                if LNaNFlag:\n",
        "                    L = EstimateL(f, xk, f(xk), grad(xk), L)\n",
        "\n",
        "                xk_plus_1 = x_bar_k - grad_bar_k / L\n",
        "                vk_plus_1, phi_star_k_plus_1 = ComputeVPhiStar(theta_k, gamma0, gamma_k_plus_1, ell, vk, phi_star_k, x_bar_k, f(x_bar_k), grad_bar_k)\n",
        "\n",
        "                if iag % 8 == 0 and f(xk_plus_1) <= f(x_bar_k) - (4 / 5) * np.dot(grad_bar_k, grad_bar_k + grad(xk_plus_1)) / (2 * L):\n",
        "                    fk_plus_1 = f(xk_plus_1)\n",
        "                    gradk_plus_1 = grad(xk_plus_1)\n",
        "                    pk = -gradk_plus_1\n",
        "                    xk = xk_plus_1\n",
        "                    vk = vk_plus_1\n",
        "                    phi_star_k = phi_star_k_plus_1\n",
        "                    only_ag = False\n",
        "                    break\n",
        "\n",
        "        gamma0 = gamma_k_plus_1\n",
        "\n",
        "    return xk, k, False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZUwJ1QkzH9b"
      },
      "source": [
        "# Implementación de Gradiente Conjugado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFKu0OtdzLO-"
      },
      "outputs": [],
      "source": [
        "def backtracking(alpha_ini, rho, c1, c2, xk, f, grad_f, pk, Nb):\n",
        "\n",
        "    alpha = alpha_ini\n",
        "\n",
        "    for i in range(Nb):\n",
        "        if f(xk + alpha*pk) <= f(xk) + c1*alpha*np.dot(grad_f(xk), pk) and np.dot(grad_f(xk + alpha*pk), pk) >= c2*(np.dot(grad_f(xk), pk)):\n",
        "            return alpha\n",
        "        alpha = rho*alpha\n",
        "\n",
        "    return alpha\n",
        "\n",
        "\n",
        "\n",
        "def GC_no_lineal(f, grad_f, x0, tol, N, alpha_ini, rho, c1, c2, Nb):\n",
        "\n",
        "    nr = 0\n",
        "    g0 = grad_f(x0)\n",
        "    d0 = -g0\n",
        "\n",
        "    gk = g0\n",
        "    xk = x0\n",
        "    dk = d0\n",
        "\n",
        "    for k in range(N):\n",
        "        xk_prev = xk\n",
        "        gk_prev = gk\n",
        "        if np.linalg.norm(gk) < tol:\n",
        "            return xk, gk, k, nr, True\n",
        "\n",
        "        alpha = backtracking(alpha_ini, rho, c1, c2, xk, f, grad_f, dk, Nb)\n",
        "        xk = xk_prev + alpha*dk\n",
        "        fk = f(xk)\n",
        "        gk = grad_f(xk)\n",
        "        yk = gk - gk_prev\n",
        "        if np.linalg.norm(np.dot(gk, gk_prev)) < 0.2*((np.linalg.norm(gk))**2):\n",
        "            beta = np.dot(gk, gk)/np.dot(gk_prev, gk_prev)\n",
        "        else:\n",
        "            beta = 0\n",
        "            nr = nr +1\n",
        "        dk = -gk + beta*dk\n",
        "\n",
        "    return xk, gk, k, nr, False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBkLDIgxzOUS"
      },
      "source": [
        "# Implementación de Gradiente Acelerado (Nesterov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp_8hpsvzOBG"
      },
      "outputs": [],
      "source": [
        "def N_AG(f, grad_f, x0, tol, N, alpha_ini, gamma):\n",
        "    xk = x0\n",
        "    yk = x0\n",
        "    vk = np.zeros_like(x0)\n",
        "\n",
        "    for k in range(N):\n",
        "        gk = grad_f(yk)\n",
        "\n",
        "        vk = gamma * vk + alpha_ini * gk\n",
        "\n",
        "        if np.any(np.isnan(vk)):\n",
        "            print(f\"NaN encontrado en vk en la iteración {k}\")\n",
        "            break\n",
        "\n",
        "        xk_prev = xk\n",
        "        xk = yk - vk\n",
        "        yk = xk + gamma * (xk - xk_prev)\n",
        "\n",
        "        if np.linalg.norm(gk) < tol:\n",
        "            return xk, gk, k, True\n",
        "\n",
        "    return xk, gk, N, False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIkJa6i5zWqK"
      },
      "source": [
        "# Prueba 1:\n",
        "\n",
        "$f(x) = \\frac{1}{2} x^{T} A_{1} - b_{1}^{T} x$\n",
        "\n",
        " Donde A1 = nI + 1, siendo n el tamaño de x0, I la matriz identidad, 1 una matriz de 1's, y b un vector unitario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUzojUyEzXDR"
      },
      "outputs": [],
      "source": [
        "def cuadratica_1_1000(x):\n",
        "    I = np.eye(1000)\n",
        "    uno = np.ones((1000,1000))\n",
        "    A1 = 1000 * I + uno\n",
        "    b1 = np.ones(1000)\n",
        "    return 0.5 * np.dot(x.T, np.dot(A1, x)) - np.dot(b1.T, x)\n",
        "\n",
        "def grad_cuadratica_1_1000(x):\n",
        "    I = np.eye(1000)\n",
        "    uno = np.ones((1000,1000))\n",
        "    b1 = np.ones(1000)\n",
        "    A1 = 1000 * I + uno\n",
        "    return 0.5*(np.dot(A1.T, x) + np.dot(A1,x)) - b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvhkqKo3zf-Q"
      },
      "source": [
        "# Solución 1 con GC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxCdUKAmzd3n",
        "outputId": "41f96faa-a645-42c9-f7d2-663eaf2fd68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  1000\n",
            "f(x0) =  0.0\n",
            "Número de iteraciones realizadas:  251\n",
            "f(xk) =  -0.24999999999146527\n",
            "Entrada  0  de xk:  0.0005000029213602938\n",
            "Entrada  1  de xk:  0.0005000029213602938\n",
            "Entrada  2  de xk:  0.0005000029213602938\n",
            "Entrada  3  de xk:  0.0005000029213602938\n",
            "Entrada  997 de xk:  0.0005000029213602938\n",
            "Entrada  998 de xk:  0.0005000029213602938\n",
            "Entrada  999 de xk:  0.0005000029213602938\n",
            "Entrada  1000 de xk:  0.0005000029213602938\n",
            "Norma de gk:  0.00018476304800255685\n",
            "¿Se cumplió la condición de paro? True\n",
            "Número de reinicios nr:  251\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "n = 1000\n",
        "N = 5000\n",
        "rho = 0.5\n",
        "c1 = 0.001\n",
        "c2 = 0.01\n",
        "Nb = 500\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(1000)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "\n",
        "xk, gk, k, nr, bres, = GC_no_lineal(cuadratica_1_1000, grad_cuadratica_1_1000, x0, tol, N, alpha_ini, rho, c1, c2, Nb)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"Número de iteraciones realizadas: \", k)\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", xk[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", xk[i])\n",
        "\n",
        "print(\"¿Se cumplió la condición de paro?\", bres)\n",
        "print(\"Número de reinicios nr: \", nr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV7V3Swj0KPV"
      },
      "source": [
        "# Solución 1 con AG (Nesterov):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3En2k2hRzh5L",
        "outputId": "3228398e-d385-4cc1-f8e4-cd01cb54a708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  1000\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.0005000052710371406\n",
            "Entrada  1  de xk:  0.0005000052710371406\n",
            "Entrada  2  de xk:  0.0005000052710371406\n",
            "Entrada  3  de xk:  0.0005000052710371406\n",
            "Entrada  997 de xk:  0.0005000052710371406\n",
            "Entrada  998 de xk:  0.0005000052710371406\n",
            "Entrada  999 de xk:  0.0005000052710371406\n",
            "Entrada  1000 de xk:  0.0005000052710371406\n",
            "\n",
            "\n",
            "Norma de gk: 5.801187865114948e-05\n",
            "Iteraciones realizadas: 44\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "n = 1000\n",
        "N = 5000\n",
        "gamma = 0.5\n",
        "alpha_ini = 0.0001\n",
        "x0 = np.zeros(1000)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "# Ejecutar el método de Nesterov\n",
        "x_opt, grad_opt, iter, bres = N_AG(cuadratica_1_1000, grad_cuadratica_1_1000, x0, tol, N, alpha_ini, gamma)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iteraciones realizadas: {iter}\")\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26gBKb9W0z0H"
      },
      "source": [
        "# Solución 1 con G+AC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd428baP0wH3",
        "outputId": "c501e26e-8c04-42bd-f24e-d38c0e13ac5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  1000\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.0005\n",
            "Entrada  1  de xk:  0.0005\n",
            "Entrada  2  de xk:  0.0005\n",
            "Entrada  3  de xk:  0.0005\n",
            "Entrada  997 de xk:  0.0005\n",
            "Entrada  998 de xk:  0.0005\n",
            "Entrada  999 de xk:  0.0005\n",
            "Entrada  1000 de xk:  0.0005\n",
            "Iteraciones realizadas:  0\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "n = 1000\n",
        "x0 = np.zeros(n)\n",
        "L = float('nan')  # Valor inicial para L\n",
        "l = 0\n",
        "gtol = 1e-6\n",
        "\n",
        "\n",
        "# Llamada a la función de optimización\n",
        "x_opt, iter, bres = CPlusAG(cuadratica_1_1000, grad_cuadratica_1_1000, x0, l, L, gtol)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "# Número de iteraciones realizadas\n",
        "print('Iteraciones realizadas: ', iter)\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ST_AmY41WAD"
      },
      "source": [
        "# Prueba 2:\n",
        "\n",
        "$f(x) = \\frac{1}{2} x^{T} A_{2} - b_{2}^{T} x$\n",
        "\n",
        " donde $a_{ij} = exp(-0.25(i-j)^{2})$ y b es un vector unitario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq_UDd8c1LTA"
      },
      "outputs": [],
      "source": [
        "def cuadratica_2_10(x):\n",
        "  A2 = np.empty([10,10], dtype = float)\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      A2[i][j] = np.exp(-0.25*(i-j)**2)\n",
        "  b2 = np.ones(10)\n",
        "  return 0.5 * np.dot(x.T, np.dot(A2, x)) - np.dot(b2.T, x)\n",
        "\n",
        "def grad_cuadratica_2_10(x):\n",
        "  A2 = np.empty([10,10], dtype = float)\n",
        "  b2 = np.ones(10)\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      A2[i][j] = np.exp(-0.25*(i-j)**2)\n",
        "  return 0.5*(np.dot(A2.T, x) + np.dot(A2,x)) - b2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ya-uAz1a9k"
      },
      "source": [
        "# Solución 2 con GC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi7QXYV41Zdz",
        "outputId": "bd35ab2e-de10-45b4-881d-d3fb8a5cdb1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  10\n",
            "Número de iteraciones realizadas:  1571\n",
            "Entrada  0  de xk:  1.3688956634168\n",
            "Entrada  1  de xk:  -1.165862919253239\n",
            "Entrada  2  de xk:  1.6083890463638981\n",
            "Entrada  3  de xk:  -0.6127911466676736\n",
            "Entrada  7 de xk:  -0.6127911466676736\n",
            "Entrada  8 de xk:  1.6083890463638981\n",
            "Entrada  9 de xk:  -1.165862919253239\n",
            "Entrada  10 de xk:  1.3688956634168\n",
            "¿Se cumplió la condición de paro? True\n",
            "Número de reinicios nr:  1230\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "n = 10\n",
        "N = 5000\n",
        "rho = 0.5\n",
        "c1 = 0.001\n",
        "c2 = 0.01\n",
        "Nb = 500\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(10)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "\n",
        "xk, gk, k, nr, bres, = GC_no_lineal(cuadratica_2_10, grad_cuadratica_2_10, x0, tol, N, alpha_ini, rho, c1, c2, Nb)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"Número de iteraciones realizadas: \", k)\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", xk[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", xk[i])\n",
        "\n",
        "print(\"¿Se cumplió la condición de paro?\", bres)\n",
        "print(\"Número de reinicios nr: \", nr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_XF8VhJGCx4"
      },
      "source": [
        "# Solución 2 con AG (Nesterov):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY10ClEK1o0P",
        "outputId": "638c30b8-b80c-48be-d681-7073d0d5608e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  10\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  1.3688281296820384\n",
            "Entrada  1  de xk:  -1.1656955388134709\n",
            "Entrada  2  de xk:  1.6081598371057975\n",
            "Entrada  3  de xk:  -0.6125963884633056\n",
            "Entrada  7 de xk:  -0.6125963884633056\n",
            "Entrada  8 de xk:  1.6081598371057975\n",
            "Entrada  9 de xk:  -1.1656955388134709\n",
            "Entrada  10 de xk:  1.3688281296820384\n",
            "\n",
            "\n",
            "Iteraciones realizadas: 1967\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "n = 10\n",
        "N = 5000\n",
        "gamma = 0.5\n",
        "alpha_ini = 0.1\n",
        "x0 = np.zeros(10)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "# Ejecutar el método de Nesterov\n",
        "x_opt, grad_opt, iter, bres = N_AG(cuadratica_2_10, grad_cuadratica_2_10, x0, tol, N, alpha_ini, gamma)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iteraciones realizadas: {iter}\")\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEROCuRkGsoe"
      },
      "source": [
        "# Solución 2 con G+AC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90_iL1UcGUom",
        "outputId": "f2b81fee-bd2a-430c-dfd8-d0d0c25ad14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  10\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  1.3690991585450476\n",
            "Entrada  1  de xk:  -1.1663768171766733\n",
            "Entrada  2  de xk:  1.6090828050261778\n",
            "Entrada  3  de xk:  -0.6133905258822363\n",
            "Entrada  7 de xk:  -0.6133905258822363\n",
            "Entrada  8 de xk:  1.6090828050261778\n",
            "Entrada  9 de xk:  -1.1663768171766733\n",
            "Entrada  10 de xk:  1.3690991585450476\n",
            "Iteraciones realizadas:  4\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "n = 10\n",
        "x0 = np.zeros(n)\n",
        "L = float('nan')  # Valor inicial para L\n",
        "l = 0\n",
        "gtol = 1e-6\n",
        "\n",
        "\n",
        "# Llamada a la función de optimización\n",
        "x_opt, iter, bres = CPlusAG(cuadratica_2_10, grad_cuadratica_2_10, x0, l, L, gtol)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "# Número de iteraciones realizadas\n",
        "print('Iteraciones realizadas: ', iter)\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmA2Ss25G6Ce"
      },
      "source": [
        "# Prueba 3: Basis Pursuit Denoising\n",
        "\n",
        "$min \\frac{1}{2} ||Ax - b||^{2} + \\lambda \\sum_{i=1}^{n} \\sqrt{x_{i}^{2} + \\delta}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kas4LaKXG1AN"
      },
      "outputs": [],
      "source": [
        "def abpdn_function(x, A, b, lambd, delta):\n",
        "    Ax_minus_b = A @ x - b\n",
        "    smooth_l1_term = np.sum(np.sqrt(x**2 + delta))\n",
        "    return 0.5 * np.dot(Ax_minus_b, Ax_minus_b) + lambd * smooth_l1_term\n",
        "\n",
        "def abpdn_gradient(x, A, b, lambd, delta):\n",
        "    Ax_minus_b = A @ x - b\n",
        "    smooth_l1_grad = x / np.sqrt(x**2 + delta)\n",
        "    return A.T @ Ax_minus_b + lambd * smooth_l1_grad\n",
        "\n",
        "# Función que genera la matriz utilizada por ABPDN\n",
        "def create_abpdn_matrices(n, m, delta):\n",
        "    A = np.diag(np.concatenate([np.ones(m), np.ones(n - m) * 1000]))\n",
        "    b = np.sin(np.arange(1, n + 1))\n",
        "    return A, b\n",
        "\n",
        "# Asignamos los parámetros a la función ABPDN y creamos una función auxiliar para llamar usando solo x\n",
        "n = 1000\n",
        "m = 500\n",
        "delta = 1e-5\n",
        "lambd = 1e-3\n",
        "A, b = create_abpdn_matrices(n, m, delta)\n",
        "\n",
        "def test_abpdn_function(x):\n",
        "    return abpdn_function(x, A, b, lambd, delta)\n",
        "\n",
        "def test_abpdn_gradient(x):\n",
        "    return abpdn_gradient(x, A, b, lambd, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58-FViiHTxF"
      },
      "source": [
        "# Solución 3 con GC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roNRaTA4HQag",
        "outputId": "00ddf0f2-6c92-432d-d213-8b49351c1ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  1000\n",
            "Número de iteraciones realizadas:  1312\n",
            "Entrada  0  de xk:  0.8404707675021708\n",
            "Entrada  1  de xk:  0.9082971904525491\n",
            "Entrada  2  de xk:  0.14012022462607937\n",
            "Entrada  3  de xk:  -0.7558023022086048\n",
            "Entrada  997 de xk:  -0.7558023022086048\n",
            "Entrada  998 de xk:  0.14012022462607937\n",
            "Entrada  999 de xk:  0.9082971904525491\n",
            "Entrada  1000 de xk:  0.8404707675021708\n",
            "¿Se cumplió la condición de paro? True\n",
            "Número de reinicios nr:  1274\n"
          ]
        }
      ],
      "source": [
        "n = 1000\n",
        "N = 5000\n",
        "rho = 0.5\n",
        "c1 = 0.001\n",
        "c2 = 0.01\n",
        "Nb = 500\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(1000)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "\n",
        "xk, gk, k, nr, bres, = GC_no_lineal(test_abpdn_function, test_abpdn_gradient, x0, tol, N, alpha_ini, rho, c1, c2, Nb)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"Número de iteraciones realizadas: \", k)\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", xk[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", xk[i])\n",
        "\n",
        "print(\"¿Se cumplió la condición de paro?\", bres)\n",
        "print(\"Número de reinicios nr: \", nr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMsamlfQIgep"
      },
      "source": [
        "# Solución 3 con AG (Nesterov):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqke43neHYL5",
        "outputId": "a16ebeda-abdb-4ffa-f97a-9aa35ff31bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN encontrado en vk en la iteración 60\n",
            "Dimension:  1000\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.8404711418946417\n",
            "Entrada  1  de xk:  0.9082975950173724\n",
            "Entrada  2  de xk:  0.1401202874572304\n",
            "Entrada  3  de xk:  -0.7558026389365773\n",
            "Entrada  997 de xk:  -0.7558026389365773\n",
            "Entrada  998 de xk:  0.1401202874572304\n",
            "Entrada  999 de xk:  0.9082975950173724\n",
            "Entrada  1000 de xk:  0.8404711418946417\n",
            "\n",
            "\n",
            "Iteraciones realizadas: 5000\n",
            "¿Se cumplió la condición de paro?: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-62c4d16a9cca>:8: RuntimeWarning: overflow encountered in square\n",
            "  smooth_l1_grad = x / np.sqrt(x**2 + delta)\n",
            "<ipython-input-17-62c4d16a9cca>:7: RuntimeWarning: invalid value encountered in matmul\n",
            "  Ax_minus_b = A @ x - b\n",
            "<ipython-input-17-62c4d16a9cca>:8: RuntimeWarning: invalid value encountered in divide\n",
            "  smooth_l1_grad = x / np.sqrt(x**2 + delta)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "n = 1000\n",
        "N = 5000\n",
        "gamma = 0.5\n",
        "alpha_ini = 0.1\n",
        "x0 = np.zeros(1000)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "# Ejecutar el método de Nesterov\n",
        "x_opt, grad_opt, iter, bres = N_AG(test_abpdn_function, test_abpdn_gradient, x0, tol, N, alpha_ini, gamma)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iteraciones realizadas: {iter}\")\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFi8dFmAJTII"
      },
      "source": [
        "# Solución 3 con C+AG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VPTaPMuIti3",
        "outputId": "d1dc1ae6-4f5a-4758-b242-d329ee80a27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  1000\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.8404709799544308\n",
            "Entrada  1  de xk:  0.9082974204823013\n",
            "Entrada  2  de xk:  0.14012025557811464\n",
            "Entrada  3  de xk:  -0.7558024927186807\n",
            "Entrada  997 de xk:  -0.7558024927186807\n",
            "Entrada  998 de xk:  0.14012025557811464\n",
            "Entrada  999 de xk:  0.9082974204823013\n",
            "Entrada  1000 de xk:  0.8404709799544308\n",
            "Iteraciones realizadas:  10\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "n = 1000\n",
        "x0 = np.zeros(n)\n",
        "L = float('nan')  # Valor inicial para L\n",
        "l = 0\n",
        "gtol = 1e-6\n",
        "\n",
        "\n",
        "# Llamada a la función de optimización\n",
        "x_opt, iter, bres = CPlusAG(test_abpdn_function, test_abpdn_gradient, x0, l, L, gtol)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "# Número de iteraciones realizadas\n",
        "print('Iteraciones realizadas: ', iter)\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtH8kkMCKXEi"
      },
      "source": [
        "# Prueba 4: Función de perdida logística\n",
        "\n",
        " $R(Ax) + \\lambda ||x||^{2}/2 $\n",
        "\n",
        "donde $R(v) = \\sum_{i=1}^{m} r(v_{i})$\n",
        "\n",
        "y $r(v_{i}) = ln(1 + e^{-v})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zz7usURMkKs"
      },
      "outputs": [],
      "source": [
        "# Función logística\n",
        "def R(v):\n",
        "    return np.sum(np.log(1 + np.exp(-v)))\n",
        "\n",
        "# Función de pérdida logística regularizada\n",
        "def logistic_loss_function(x, A, lambd):\n",
        "    return R(A @ x) + (lambd / 2) * np.linalg.norm(x) ** 2\n",
        "\n",
        "# Gradiente de la función logística\n",
        "def grad_R(v):\n",
        "    return -1 / (1 + np.exp(v))\n",
        "\n",
        "# Gradiente de la función de pérdida logística regularizada\n",
        "def logistic_loss_grad(x, A, lambd):\n",
        "    return A.T @ grad_R(A @ x) + lambd * x\n",
        "\n",
        "\n",
        "# Definimos el tamaño de la matriz\n",
        "m = 100\n",
        "n = 50\n",
        "\n",
        "# Fijamos la semilla del generador de números aleatorios para reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Vector de unos\n",
        "e = np.ones(n)\n",
        "\n",
        "# Generamos la matriz A\n",
        "sigma = 0.4\n",
        "A = np.zeros((m, n))\n",
        "for i in range(m):\n",
        "    z_i = np.random.normal(0, sigma, n)\n",
        "    A[i, :] = e / np.sqrt(n) + z_i\n",
        "\n",
        "# Parámetros de regularización\n",
        "lambd1 = 1e-4\n",
        "\n",
        "def test_logloss_function(x):\n",
        "  return logistic_loss_function(x, A, lambd)\n",
        "\n",
        "def test_logloss_grad(x):\n",
        "  return logistic_loss_grad(x, A, lambd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxLUlCzlLA5Z"
      },
      "source": [
        "# Solución 4 con GC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQ3l553LCP6",
        "outputId": "9bf426bb-80b2-45f7-a86f-efe7771f07fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  50\n",
            "Número de iteraciones realizadas:  4999\n",
            "Entrada  0  de xk:  2.2988868051911586\n",
            "Entrada  1  de xk:  6.478331320454311\n",
            "Entrada  2  de xk:  8.669251539813537\n",
            "Entrada  3  de xk:  8.064267655852795\n",
            "Entrada  47 de xk:  8.064267655852795\n",
            "Entrada  48 de xk:  8.669251539813537\n",
            "Entrada  49 de xk:  6.478331320454311\n",
            "Entrada  50 de xk:  2.2988868051911586\n",
            "¿Se cumplió la condición de paro? False\n",
            "Número de reinicios nr:  5000\n"
          ]
        }
      ],
      "source": [
        "N = 5000\n",
        "rho = 0.5\n",
        "c1 = 0.001\n",
        "c2 = 0.01\n",
        "Nb = 500\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(50)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "\n",
        "xk, gk, k, nr, bres, = GC_no_lineal(test_logloss_function, test_logloss_grad, x0, tol, N, alpha_ini, rho, c1, c2, Nb)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"Número de iteraciones realizadas: \", k)\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", xk[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", xk[i])\n",
        "\n",
        "print(\"¿Se cumplió la condición de paro?\", bres)\n",
        "print(\"Número de reinicios nr: \", nr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5yL1u1R64m"
      },
      "source": [
        "# Solución 4 con AG (Nesterov):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5793kgILb_E",
        "outputId": "f91fcbc8-86c7-4a53-9ab8-68107fb31239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  50\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  -1.5365069011796482\n",
            "Entrada  1  de xk:  0.8977476876601626\n",
            "Entrada  2  de xk:  2.562964241289959\n",
            "Entrada  3  de xk:  2.7041663813147796\n",
            "Entrada  47 de xk:  2.7041663813147796\n",
            "Entrada  48 de xk:  2.562964241289959\n",
            "Entrada  49 de xk:  0.8977476876601626\n",
            "Entrada  50 de xk:  -1.5365069011796482\n",
            "\n",
            "\n",
            "Iteraciones realizadas: 11758\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "N = 50000\n",
        "gamma = 0.5\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(n)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "# Ejecutar el método de Nesterov\n",
        "x_opt, grad_opt, iter, bres = N_AG(test_logloss_function, test_logloss_grad, x0, tol, N, alpha_ini, gamma)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iteraciones realizadas: {iter}\")\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG5d_AE8R9Dp"
      },
      "source": [
        "# Solución 4 con C+AG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KSh7qa9S8gW",
        "outputId": "093de992-9968-4428-c001-ba0eb2173998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  50\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  -1.608055487676299\n",
            "Entrada  1  de xk:  0.8901739574556402\n",
            "Entrada  2  de xk:  2.5841621945415425\n",
            "Entrada  3  de xk:  2.648803954952745\n",
            "Entrada  47 de xk:  2.648803954952745\n",
            "Entrada  48 de xk:  2.5841621945415425\n",
            "Entrada  49 de xk:  0.8901739574556402\n",
            "Entrada  50 de xk:  -1.608055487676299\n",
            "Iteraciones realizadas:  24\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "x0 = np.zeros(n)\n",
        "L = float('nan')  # Valor inicial para L\n",
        "l = 0\n",
        "gtol = 1e-6\n",
        "\n",
        "# Llamada a la función de optimización\n",
        "x_opt, iter, bres = CPlusAG(test_logloss_function, test_logloss_grad, x0, l, L, gtol)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "# Número de iteraciones realizadas\n",
        "print('Iteraciones realizadas: ', iter)\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdYMZ3UOVTvm"
      },
      "source": [
        "# Prueba 5: Regresión de Huber\n",
        "\n",
        "  $ \\zeta(t) = \\begin{cases}\n",
        "      - \\tau^{2} - \\tau t & si \\; t \\leq \\tau \\\\\n",
        "      t^{2} & si \\; t \\in [-\\tau, \\tau]  \\\\\n",
        "      -\\tau^{2} + 2 \\tau t &  si \\; t \\geq \\tau\n",
        "   \\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HvXAb0SVdod"
      },
      "outputs": [],
      "source": [
        "# Definimos la función de la regresión de Huber y su gradiente\n",
        "def huber_regression_function(x, A, b, tau):\n",
        "    Ax_minus_b = A @ x - b\n",
        "    zeta = np.where(np.abs(Ax_minus_b) <= tau,\n",
        "                    Ax_minus_b**2,\n",
        "                    tau * (2 * np.abs(Ax_minus_b) - tau))\n",
        "    return np.sum(zeta)\n",
        "\n",
        "def huber_regression_gradient(x, A, b, tau):\n",
        "    Ax_minus_b = A @ x - b\n",
        "    grad_zeta = np.where(np.abs(Ax_minus_b) <= tau,\n",
        "                         2 * Ax_minus_b,\n",
        "                         tau * np.sign(Ax_minus_b))\n",
        "    return A.T @ grad_zeta\n",
        "\n",
        "# Función para crear la matriz A y el vector b para la función de HUber\n",
        "def create_huber_matrices(n):\n",
        "    A = np.eye(n + 1) - np.eye(n + 1, k=-1)\n",
        "    b = np.ones(n + 1)\n",
        "    b[-1] = -1.1 * n\n",
        "    return A, b\n",
        "\n",
        "n = 100\n",
        "tau = 250\n",
        "A, b = create_huber_matrices(n)\n",
        "\n",
        "def test_huber_regression_function(x):\n",
        "    return huber_regression_function(x, A, b, tau)\n",
        "\n",
        "def test_huber_regression_gradient(x):\n",
        "    return huber_regression_gradient(x, A, b, tau)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfXbGsFVa9G"
      },
      "source": [
        "# Solución 5 con GC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0OvCwNVVdUq",
        "outputId": "6a50161e-ff2b-4381-8dcf-c00b12495571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  100\n",
            "Número de iteraciones realizadas:  50000\n",
            "Entrada  0  de xk:  0.9924884935265623\n",
            "Entrada  1  de xk:  1.9849690980029322\n",
            "Entrada  2  de xk:  2.977472669103885\n",
            "Entrada  3  de xk:  3.9699526084454906\n",
            "Entrada  97 de xk:  3.9699526084454906\n",
            "Entrada  98 de xk:  2.977472669103885\n",
            "Entrada  99 de xk:  1.9849690980029322\n",
            "Entrada  100 de xk:  0.9924884935265623\n",
            "¿Se cumplió la condición de paro? False\n",
            "Número de reinicios nr:  42620\n"
          ]
        }
      ],
      "source": [
        "N = 50001\n",
        "rho = 0.5\n",
        "c1 = 0.001\n",
        "c2 = 0.1\n",
        "Nb = 500\n",
        "alpha_ini = 1\n",
        "x0 = np.zeros(n + 1)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "\n",
        "xk, gk, k, nr, bres, = GC_no_lineal(test_huber_regression_function, test_huber_regression_gradient, x0, tol, N, alpha_ini, rho, c1, c2, Nb)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"Número de iteraciones realizadas: \", k)\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", xk[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", xk[i])\n",
        "\n",
        "print(\"¿Se cumplió la condición de paro?\", bres)\n",
        "print(\"Número de reinicios nr: \", nr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emzbBdFCcE13"
      },
      "source": [
        "# Solución 5 con AG (Nesterov):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCsrRo2UcEVm",
        "outputId": "be90e13d-4768-4d50-e4a9-361e08f4ab4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  100\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.9997254407063478\n",
            "Entrada  1  de xk:  1.9994509471686597\n",
            "Entrada  2  de xk:  2.9991765851271515\n",
            "Entrada  3  de xk:  3.9989024202905465\n",
            "Entrada  97 de xk:  3.9989024202905465\n",
            "Entrada  98 de xk:  2.9991765851271515\n",
            "Entrada  99 de xk:  1.9994509471686597\n",
            "Entrada  100 de xk:  0.9997254407063478\n",
            "\n",
            "\n",
            "Iteraciones realizadas: 43904\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Resolvemos para la funcion cuadratica en R10\n",
        "\n",
        "N = 50000\n",
        "gamma = 0.5\n",
        "alpha_ini = 0.1\n",
        "x0 = np.zeros(n + 1)\n",
        "epsilon = sys.float_info.epsilon\n",
        "tol = np.sqrt(n)*(epsilon**(1/3))\n",
        "# Ejecutar el método de Nesterov\n",
        "x_opt, grad_opt, iter, bres = N_AG(test_huber_regression_function, test_huber_regression_gradient, x0, tol, N, alpha_ini, gamma)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Iteraciones realizadas: {iter}\")\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4AwwbdcCGy"
      },
      "source": [
        "# Solución 5 con C+AG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jhRN8UpSOcc",
        "outputId": "afdff2e1-b94f-4867-db68-4c9f5e0e9d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension:  100\n",
            "\n",
            "\n",
            "Entrada  0  de xk:  0.9999999999998714\n",
            "Entrada  1  de xk:  1.9999999999997242\n",
            "Entrada  2  de xk:  3.0000000000003957\n",
            "Entrada  3  de xk:  3.9999999999996247\n",
            "Entrada  97 de xk:  3.9999999999996247\n",
            "Entrada  98 de xk:  3.0000000000003957\n",
            "Entrada  99 de xk:  1.9999999999997242\n",
            "Entrada  100 de xk:  0.9999999999998714\n",
            "Iteraciones realizadas:  100\n",
            "¿Se cumplió la condición de paro?: True\n"
          ]
        }
      ],
      "source": [
        "x0 = np.zeros(n + 1)\n",
        "\n",
        "x_opt, iter, bres = CPlusAG(test_huber_regression_function, test_huber_regression_gradient, x0)\n",
        "\n",
        "print(\"Dimension: \", n)\n",
        "print(\"\\n\")\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Entrada \", i, \" de xk: \", x_opt[i])\n",
        "\n",
        "for i in range(3, -1, -1):\n",
        "    print(\"Entrada \", n - i, \"de xk: \", x_opt[i])\n",
        "\n",
        "# Número de iteraciones realizadas\n",
        "print('Iteraciones realizadas: ', iter)\n",
        "print(f\"¿Se cumplió la condición de paro?: {bres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjUC67IrlimY"
      },
      "source": [
        "# Resumen de resultados\n",
        "\n",
        "\n",
        "| Función               | Gradiente Conjugado | Gradiente Acelerado | C+AG |\n",
        "|-----------------------|---------------------|---------------------|------|\n",
        "| Cuadrática 1             | 251 (T)            | 44 (T)             | 0 (T) |\n",
        "| Cuadrática 2             | 1571 (T)           | 1967  (T)           | 4 (T) |\n",
        "| ABPD                  | 1312 (T)            | 5000 (F)            | 10 (T)|\n",
        "| Logistic Loss (LL)    | 5000 (F)            | 11758 (T)            | 24 (T)|\n",
        "| Huber Regression (HR) | 50000 (F)            | 43904 (T)            | 100 (T) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ueaKVbHbZp4"
      },
      "source": [
        "# Conclusiones\n",
        "\n",
        "Podemos ver que para las 5 funciones de prueba aplicadas, el método C+AG del artículo supera considerablemente a los métodos de CG y AG en número de iteraciones necesarias para alcanzar el mínimo de la función, cumpliendo nuestro objetivo al implementar el método de la publicación.\n",
        "\n",
        "Aunque los métodos de CG y AC no convergieron para todas las pruebas, en realidad solo en la prueba de la función de perdida logística, el método de CG no se acercó lo suficiente al mínimo de la función (significando que requería un considerable número mayor de iteraciones, pero se decidió dejar así por cuestiones de economizar tiempo), en los demás casos CG para HR y AG para ABPD, los mínimos obtenidos en la última iteración fueron bastante cercanos al mínimo de la función, significando que solo requerían un poco más de iteraciones para alcanzar el valor buscado.\n",
        "\n",
        "En cuánto a discrepancias con los resultados del artículo, solo tenemos una, que es para el caso de la función de perdida logística, en los resultados del artículo converge más rápido para CG que para C+AG, sin embargo en nuestra implementación ocurrió lo contrario, esto puede deberse a las diferencias en la implementación del método de CG, siendo la función utilizada por los autores del artículo una versión más sofisticada del método, podría ser interesante repetir estas pruebas con una versión implementada de CG en una librería para hacer una comparación un poco más \"justa\".\n",
        "\n",
        "Además, se esperaba que para el caso de las primeras dos funciones cuadráticas el método de CG superara al método de AG pero ocurrió que en la función 1 el método de AG fue más rápido, lo cuál nos pareció extraño considerando que CG es en teoría el método óptimo para la solución de funciones cuadráticas, de nuevo esto podría estar relacionado con nuestra implementación del método de CG.\n",
        "\n",
        "Sin embargo, consideramos que en general los resultados son bastante positivos, esperabamos que el método del artículo superara en eficiencia a los métodos de CG y AG para corroborar que logra cerrar la brecha entre los dos métodos, y efectivamente lo logró, resultados que en general además concuerdan con los obtenidos por los autores en el artículo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TilNkCfDd0Uv"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "[1]  Nonlinear conjugate gradient for smooth convex\n",
        "functions. Sahar Karimi, Stephen Vavasis.\n",
        "\n",
        "https://arxiv.org/pdf/2111.11613"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
